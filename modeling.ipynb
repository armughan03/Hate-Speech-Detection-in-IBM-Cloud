{"cells": [{"metadata": {}, "cell_type": "code", "source": "from project_lib import Project\nproject = Project(spark.sparkContext, 'adcc95ed-0e6f-494f-a3a1-d5e74e038455', 'p-0e7c11d93da5347766da7d0e0924866399a71d41')\npc = project.project_context", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200615132334-0003\nKERNEL_ID = 7f125647-852b-46ad-b1ef-cd57b7c4c0db\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import ibmos2spark\nimport nltk\n!pip install numpy\n!pip install gensim \nfrom gensim.parsing.preprocessing import STOPWORDS", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "Collecting numpy\n  Using cached https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.18.5\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/numpy-1.18.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/bin already exists. Specify --upgrade to force replacement.\u001b[0m\nCollecting gensim\n  Using cached https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl\nCollecting smart-open>=1.8.1 (from gensim)\nCollecting scipy>=0.18.1 (from gensim)\n  Using cached https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl\nCollecting numpy>=1.11.3 (from gensim)\n  Using cached https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl\nCollecting six>=1.5.0 (from gensim)\n  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\nCollecting boto (from smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl\nCollecting boto3 (from smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/9d/31/f1019ce0a6e4684b5462c70ec2972adce248425da6e74cd6e03015297445/boto3-1.14.2-py2.py3-none-any.whl\nCollecting requests (from smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl\nCollecting s3transfer<0.4.0,>=0.3.0 (from boto3->smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\nCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\nCollecting botocore<1.18.0,>=1.17.2 (from boto3->smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/65/5c/8e001190db89151347c497e8faa6fc1059acc8c0079b9adffa1f0ee2fe82/botocore-1.17.2-py2.py3-none-any.whl\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests->smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/98/99/def511020aa8f663d4a2cfaa38467539e864799289ff354569e339e375b1/certifi-2020.4.5.2-py2.py3-none-any.whl\nCollecting chardet<4,>=3.0.2 (from requests->smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\nCollecting idna<3,>=2.5 (from requests->smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl\nCollecting python-dateutil<3.0.0,>=2.1 (from botocore<1.18.0,>=1.17.2->boto3->smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\nCollecting docutils<0.16,>=0.10 (from botocore<1.18.0,>=1.17.2->boto3->smart-open>=1.8.1->gensim)\n  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n\u001b[31mpytest-openfiles 0.5.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mibm-cos-sdk-core 2.6.0 has requirement requests<2.23,>=2.18, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\nInstalling collected packages: boto, six, python-dateutil, jmespath, docutils, urllib3, botocore, s3transfer, boto3, certifi, chardet, idna, requests, smart-open, numpy, scipy, gensim\nSuccessfully installed boto-2.49.0 boto3-1.14.2 botocore-1.17.2 certifi-2020.4.5.2 chardet-3.0.4 docutils-0.15.2 gensim-3.8.3 idna-2.9 jmespath-0.10.0 numpy-1.18.5 python-dateutil-2.8.1 requests-2.23.0 s3transfer-0.3.3 scipy-1.4.1 six-1.15.0 smart-open-2.0.0 urllib3-1.25.9\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/botocore-1.17.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/chardet already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/python_dateutil-2.8.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/dateutil already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/smart_open-2.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/numpy-1.18.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/urllib3-1.25.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/smart_open already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/chardet-3.0.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/docutils-0.15.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/jmespath-0.10.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests-2.23.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/s3transfer-0.3.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/scipy-1.4.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/certifi-2020.4.5.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/six-1.15.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/boto3-1.14.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/s3transfer already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/docutils already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/gensim-3.8.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/gensim already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/jmespath already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/botocore already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/idna-2.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/boto-2.49.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/boto already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/boto3 already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n", "name": "stdout"}, {"output_type": "error", "ename": "ModuleNotFoundError", "evalue": "No module named 'numpy.version'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "\u001b[0;32m<ipython-input-9-23014f6efe24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gensim '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/home/spark/shared/user-libs/python3.6/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/spark/shared/user-libs/python3.6/gensim/parsing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n\u001b[0m\u001b[1;32m      5\u001b[0m                             \u001b[0mstrip_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_numeric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mstrip_non_alphanum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_multiple_whitespaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/spark/shared/user-libs/python3.6/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/spark/shared/user-libs/python3.6/gensim/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/spark/shared/user-libs/python3.6/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgit_revision\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__git_revision__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.version'"]}]}, {"metadata": {}, "cell_type": "code", "source": "credentials = {\n    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'service_id': 'iam-ServiceId-283f9a60-2385-40f6-973d-6cd059b4731f',\n    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n    'api_key': '8-VH-6DHOLwKlgloUuzelkhXgt0elMaOQxDZB1EznoCu'\n}\n\nconfiguration_name = 'os_c42173278f6a4c65aaac64664b8be689_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf_data_1 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('data.csv', 'bdaproject-donotdelete-pr-qh7hn4gnoctssn'))", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import nltk\nalphabets = []\ntokenizer = nltk.RegexpTokenizer(r\"\\w+\")\nfor i in range(97,123):\n    alphabets.append(chr(i))\ndef cleaning(line):\n    rem_garbage = line.replace(\"NEWLINE_TOKEN\",'').replace(\"_\",\" \").replace(\"1\",\" \").replace(\"2\",\" \").replace(\"3\",\" \").replace(\"4\",\" \").replace(\"5\",\" \").replace(\"6\",\" \").replace(\"7\",\" \").replace(\"8\",\" \").replace(\"9\",\" \").replace(\"0\",\" \").lower()\n    rem_punct = tokenizer.tokenize(rem_garbage)\n    rem_stopwords = [word for word in rem_punct if not word in STOPWORDS.union(set(alphabets)) or not word.isalpha()]\n    return rem_stopwords", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def create_pair(word):\n    return (word,1)", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "coments = df_data_1.select(\"comment\").rdd.collect()", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clean_coments = []\nfor i in range(len(coments)):\n    clean_coments.extend(cleaning(coments[i][0]))", "execution_count": 7, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'STOPWORDS' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-7-f4cb1d524c29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclean_coments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclean_coments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m<ipython-input-4-db35f9cf1418>\u001b[0m in \u001b[0;36mcleaning\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrem_garbage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NEWLINE_TOKEN\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"6\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"7\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"8\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"9\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrem_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrem_garbage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrem_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrem_punct\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphabets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrem_stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-4-db35f9cf1418>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrem_garbage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NEWLINE_TOKEN\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"6\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"7\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"8\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"9\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrem_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrem_garbage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrem_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrem_punct\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphabets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrem_stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'STOPWORDS' is not defined"]}]}, {"metadata": {}, "cell_type": "code", "source": "clean_coments[:10]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "cleaned_comments = sc.parallelize(clean_coments)\npaired_words = cleaned_comments.map(create_pair)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def sum_counts(a, b):\n    return a + b", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "corpus_word_count = paired_words.reduceByKey(sum_counts).collect()\nnew_df = sqlContext.createDataFrame(corpus_word_count,[\"Words\",\"Count\"]).toPandas()\nnew_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install textblob\nsentiment = []\nfrom textblob import TextBlob\nfor i in range(len(new_df)):\n    blob = TextBlob(new_df[\"Words\"][i])\n    sentiment.append((new_df[\"Words\"][i],new_df[\"Count\"][i],blob.sentiment.polarity))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nDF = pd.DataFrame(sentiment,columns=[\"Words\",\"Count\",\"Sentiment_Score\"])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "WCS_DF = DF[DF[\"Count\"]>20].sort_values(by=\"Count\",ascending = False)\nWCS_DF.index = range(len(WCS_DF[\"Words\"]))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rev_ids = df_data_1.select(df_data_1[0]).rdd.collect()\nclean_coments = []\nfor i in range(len(coments)):\n    clean_coments.append(cleaning(coments[i][0]))\ncorpus = []\nfor j in range(len(clean_coments)):\n     corpus.append(' '.join([str(elem) for elem in clean_coments[j]]))\nvocab = list(WCS_DF[\"Words\"])\nfrom sklearn.feature_extraction.text import CountVectorizer\ndf = pd.DataFrame(data=corpus, columns=['reviews'])\nvectorizer = CountVectorizer(vocabulary=vocab, min_df=0)\nX = vectorizer.fit_transform(df['reviews'].values)\nresult = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "DF.head(100)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "a = result.tail(1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "a.reset_index(inplace=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sen = \"\"\nfor i in a.columns:\n    if a[i][0] == 1:\n        print(str(i))\n        sen.join(str(i)+\" \")", "execution_count": null, "outputs": [{"output_type": "stream", "text": "mean\nquestion\ncame\nstory\ndetails\nworth\nclean\ndare\nbrother\nalleged\nwife\nryan\naffair\ninvestigating\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "sen = \":There ought to be a dry-break mechanism in the fuel boom to prevent any chance of a fire, let alone an explosion.  It's Hollywood, do you expect realism when you can have drama?  Call it cinematic licence.\"\naaa = TextBlob(sen)\nprint(aaa.sentiment.polarity)", "execution_count": null, "outputs": [{"output_type": "stream", "text": "0.0\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "my_file = project.get_file(\"label_Score_data.csv\")\n\n# Read the CSV data file from the object storage into a pandas DataFrame\nmy_file.seek(0)\nimport pandas as pd\ndata_1 = pd.read_csv(my_file)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data_1.head()", "execution_count": null, "outputs": [{"output_type": "execute_result", "execution_count": 23, "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rev_id</th>\n      <th>sentiment</th>\n      <th>sentiment_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>325467263</td>\n      <td>negative</td>\n      <td>-0.723434</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>325489890</td>\n      <td>negative</td>\n      <td>-0.973351</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>325491573</td>\n      <td>negative</td>\n      <td>-0.951228</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>325491602</td>\n      <td>negative</td>\n      <td>-0.909637</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>325493177</td>\n      <td>negative</td>\n      <td>-0.861186</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "      rev_id sentiment  sentiment_score\n0  325467263  negative        -0.723434\n1  325489890  negative        -0.973351\n2  325491573  negative        -0.951228\n3  325491602  negative        -0.909637\n4  325493177  negative        -0.861186"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "my_file = project.get_file(\"data.csv\")\n\n# Read the CSV data file from the object storage into a pandas DataFrame\nmy_file.seek(0)\ndata = pd.read_csv(my_file)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data.head()", "execution_count": null, "outputs": [{"output_type": "execute_result", "execution_count": 25, "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rev_id</th>\n      <th>comment</th>\n      <th>year</th>\n      <th>ns</th>\n      <th>logged_in</th>\n      <th>sample</th>\n      <th>Aggressive_Score</th>\n      <th>Attack_Score</th>\n      <th>Toxicity_Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>325467263</td>\n      <td>NEWLINE_TOKEN:There ought to be a dry-break me...</td>\n      <td>2009</td>\n      <td>article</td>\n      <td>True</td>\n      <td>blocked</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>325489890</td>\n      <td>NEWLINE_TOKENNEWLINE_TOKEN==I AM NOT VANDALIZI...</td>\n      <td>2009</td>\n      <td>user</td>\n      <td>True</td>\n      <td>blocked</td>\n      <td>0.555555</td>\n      <td>0.333333</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>325491573</td>\n      <td>`NEWLINE_TOKENNEWLINE_TOKEN==NO ONE CAN APPARE...</td>\n      <td>2009</td>\n      <td>article</td>\n      <td>True</td>\n      <td>blocked</td>\n      <td>0.666666</td>\n      <td>0.666666</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>325491602</td>\n      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENWikiped...</td>\n      <td>2009</td>\n      <td>user</td>\n      <td>True</td>\n      <td>blocked</td>\n      <td>0.333333</td>\n      <td>0.222222</td>\n      <td>0.3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>325493177</td>\n      <td>NEWLINE_TOKENNEWLINE_TOKEN=Speedy Deletion=NEW...</td>\n      <td>2009</td>\n      <td>article</td>\n      <td>True</td>\n      <td>blocked</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "      rev_id                                            comment  year  \\\n0  325467263  NEWLINE_TOKEN:There ought to be a dry-break me...  2009   \n1  325489890  NEWLINE_TOKENNEWLINE_TOKEN==I AM NOT VANDALIZI...  2009   \n2  325491573  `NEWLINE_TOKENNEWLINE_TOKEN==NO ONE CAN APPARE...  2009   \n3  325491602  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENWikiped...  2009   \n4  325493177  NEWLINE_TOKENNEWLINE_TOKEN=Speedy Deletion=NEW...  2009   \n\n        ns  logged_in   sample  Aggressive_Score  Attack_Score  Toxicity_Score  \n0  article       True  blocked          0.000000      0.000000             0.0  \n1     user       True  blocked          0.555555      0.333333             0.5  \n2  article       True  blocked          0.666666      0.666666             0.9  \n3     user       True  blocked          0.333333      0.222222             0.3  \n4  article       True  blocked          0.000000      0.000000             0.0  "}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "!pip install nltk\n!pip install ibm_watson\n!pip install cloudant==2.3.1\n!pip install watson-developer-cloud==1.5\n!pip install --upgrade pixiedust", "execution_count": null, "outputs": [{"output_type": "stream", "text": "Collecting nltk\nCollecting regex (from nltk)\n  Using cached https://files.pythonhosted.org/packages/1a/a1/6d8fdf4a20ffbbf2bd6003dff47a0628b9e6a4b840c421b0dec27da9376e/regex-2020.6.8-cp36-cp36m-manylinux2010_x86_64.whl\nCollecting tqdm (from nltk)\n  Using cached https://files.pythonhosted.org/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl\nCollecting click (from nltk)\n  Using cached https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl\nCollecting joblib (from nltk)\n  Using cached https://files.pythonhosted.org/packages/b8/a6/d1a816b89aa1e9e96bcb298eb1ee1854f21662ebc6d55ffa3d7b3b50122b/joblib-0.15.1-py3-none-any.whl\nInstalling collected packages: regex, tqdm, click, joblib, nltk\nSuccessfully installed click-7.1.2 joblib-0.15.1 nltk-3.5 regex-2020.6.8 tqdm-4.46.1\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/joblib-0.15.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/nltk already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/tqdm-4.46.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/nltk-3.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/regex already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/tqdm already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/click-7.1.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/click already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/regex-2020.6.8.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/bin already exists. Specify --upgrade to force replacement.\u001b[0m\nCollecting ibm_watson\nCollecting websocket-client==0.48.0 (from ibm_watson)\n  Using cached https://files.pythonhosted.org/packages/8a/a1/72ef9aa26cfe1a75cee09fc1957e4723add9de098c15719416a1ee89386b/websocket_client-0.48.0-py2.py3-none-any.whl\nCollecting ibm-cloud-sdk-core==1.5.1 (from ibm_watson)\nCollecting python-dateutil>=2.5.3 (from ibm_watson)\n  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\nCollecting requests<3.0,>=2.0 (from ibm_watson)\n  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl\nCollecting six (from websocket-client==0.48.0->ibm_watson)\n  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\nCollecting PyJWT>=1.7.1 (from ibm-cloud-sdk-core==1.5.1->ibm_watson)\n  Using cached https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\nCollecting chardet<4,>=3.0.2 (from requests<3.0,>=2.0->ibm_watson)\n  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\nCollecting idna<3,>=2.5 (from requests<3.0,>=2.0->ibm_watson)\n  Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0,>=2.0->ibm_watson)\n  Using cached https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests<3.0,>=2.0->ibm_watson)\n  Using cached https://files.pythonhosted.org/packages/98/99/def511020aa8f663d4a2cfaa38467539e864799289ff354569e339e375b1/certifi-2020.4.5.2-py2.py3-none-any.whl\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n\u001b[31mpytest-openfiles 0.5.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mibm-cos-sdk-core 2.6.0 has requirement requests<2.23,>=2.18, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\nInstalling collected packages: six, websocket-client, PyJWT, python-dateutil, chardet, idna, urllib3, certifi, requests, ibm-cloud-sdk-core, ibm-watson\nSuccessfully installed PyJWT-1.7.1 certifi-2020.4.5.2 chardet-3.0.4 ibm-cloud-sdk-core-1.5.1 ibm-watson-4.5.0 idna-2.9 python-dateutil-2.8.1 requests-2.23.0 six-1.15.0 urllib3-1.25.9 websocket-client-0.48.0\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/websocket already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/chardet already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/python_dateutil-2.8.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/dateutil already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/urllib3-1.25.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/chardet-3.0.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core-1.5.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/test already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/jwt already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/ibm_watson-4.5.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests-2.23.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/certifi-2020.4.5.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/PyJWT-1.7.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/six-1.15.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/ibm_watson already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/idna-2.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/websocket_client-0.48.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n", "name": "stdout"}, {"output_type": "stream", "text": "Collecting cloudant==2.3.1\nCollecting requests<3.0.0,>=2.7.0 (from cloudant==2.3.1)\n  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl\nCollecting idna<3,>=2.5 (from requests<3.0.0,>=2.7.0->cloudant==2.3.1)\n  Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests<3.0.0,>=2.7.0->cloudant==2.3.1)\n  Using cached https://files.pythonhosted.org/packages/98/99/def511020aa8f663d4a2cfaa38467539e864799289ff354569e339e375b1/certifi-2020.4.5.2-py2.py3-none-any.whl\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0,>=2.7.0->cloudant==2.3.1)\n  Using cached https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl\nCollecting chardet<4,>=3.0.2 (from requests<3.0.0,>=2.7.0->cloudant==2.3.1)\n  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n\u001b[31mibm-cos-sdk-core 2.6.0 has requirement requests<2.23,>=2.18, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\nInstalling collected packages: idna, certifi, urllib3, chardet, requests, cloudant\nSuccessfully installed certifi-2020.4.5.2 chardet-3.0.4 cloudant-2.3.1 idna-2.9 requests-2.23.0 urllib3-1.25.9\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/chardet already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/cloudant-2.3.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/urllib3-1.25.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/chardet-3.0.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests-2.23.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/certifi-2020.4.5.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/cloudant already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/idna-2.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/bin already exists. Specify --upgrade to force replacement.\u001b[0m\nCollecting watson-developer-cloud==1.5\nCollecting Twisted>=13.2.0 (from watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/b7/04/1a664c9e5ec0224a1c1a154ddecaa4dc7b8967521bba225efcc41a03d5f3/Twisted-20.3.0-cp36-cp36m-manylinux1_x86_64.whl\nCollecting service-identity>=17.0.0 (from watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\nCollecting pyOpenSSL>=16.2.0 (from watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl\nCollecting python-dateutil>=2.5.3 (from watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\nCollecting autobahn>=0.10.9 (from watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/18/5b/ace5503285b0df547d3b417f2020fc7d57917ac808702de1ca5bdfd4aa40/autobahn-20.6.1-py2.py3-none-any.whl\nCollecting requests<3.0,>=2.0 (from watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl\nCollecting Automat>=0.3.0 (from Twisted>=13.2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\nCollecting incremental>=16.10.1 (from Twisted>=13.2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\nCollecting PyHamcrest!=1.10.0,>=1.9.0 (from Twisted>=13.2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl\nCollecting constantly>=15.1 (from Twisted>=13.2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\nCollecting attrs>=19.2.0 (from Twisted>=13.2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\nCollecting zope.interface>=4.4.2 (from Twisted>=13.2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl\nCollecting hyperlink>=17.1.1 (from Twisted>=13.2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\nCollecting cryptography (from service-identity>=17.0.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl\nCollecting pyasn1-modules (from service-identity>=17.0.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\nCollecting pyasn1 (from service-identity>=17.0.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\nCollecting six>=1.5.2 (from pyOpenSSL>=16.2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\nCollecting txaio>=20.3.1 (from autobahn>=0.10.9->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/4f/82/0cd8d81d57e55a598cd4cef10c6e971dbcaf437e4f138dc1624cf7c1388e/txaio-20.4.1-py2.py3-none-any.whl\nCollecting idna<3,>=2.5 (from requests<3.0,>=2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests<3.0,>=2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/98/99/def511020aa8f663d4a2cfaa38467539e864799289ff354569e339e375b1/certifi-2020.4.5.2-py2.py3-none-any.whl\nCollecting chardet<4,>=3.0.2 (from requests<3.0,>=2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0,>=2.0->watson-developer-cloud==1.5)\n", "name": "stdout"}, {"output_type": "stream", "text": "  Using cached https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl\nCollecting setuptools (from zope.interface>=4.4.2->Twisted>=13.2.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/95/95/f657b6e17f00c3f35b5f68b10e46c3a43af353d8856bd57bfcfb1dbb3e92/setuptools-47.1.1-py3-none-any.whl\nCollecting cffi!=1.11.3,>=1.8 (from cryptography->service-identity>=17.0.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/f1/c7/72abda280893609e1ddfff90f8064568bd8bcb2c1770a9d5bb5edb2d1fea/cffi-1.14.0-cp36-cp36m-manylinux1_x86_64.whl\nCollecting pycparser (from cffi!=1.11.3,>=1.8->cryptography->service-identity>=17.0.0->watson-developer-cloud==1.5)\n  Using cached https://files.pythonhosted.org/packages/ae/e7/d9c3a176ca4b02024debf82342dab36efadfc5776f9c8db077e8f6e71821/pycparser-2.20-py2.py3-none-any.whl\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n\u001b[31mpytest-openfiles 0.5.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mibm-cos-sdk-core 2.6.0 has requirement requests<2.23,>=2.18, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\nInstalling collected packages: six, attrs, Automat, incremental, PyHamcrest, constantly, setuptools, zope.interface, idna, hyperlink, Twisted, pycparser, cffi, cryptography, pyasn1, pyasn1-modules, service-identity, pyOpenSSL, python-dateutil, txaio, autobahn, certifi, chardet, urllib3, requests, watson-developer-cloud\nSuccessfully installed Automat-20.2.0 PyHamcrest-2.0.2 Twisted-20.3.0 attrs-19.3.0 autobahn-20.6.1 certifi-2020.4.5.2 cffi-1.14.0 chardet-3.0.4 constantly-15.1.0 cryptography-2.9.2 hyperlink-19.0.0 idna-2.9 incremental-17.5.0 pyOpenSSL-19.1.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 python-dateutil-2.8.1 requests-2.23.0 service-identity-18.1.0 setuptools-47.1.1 six-1.15.0 txaio-20.4.1 urllib3-1.25.9 watson-developer-cloud-1.5.0 zope.interface-5.1.0\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/hyperlink already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/attrs-19.3.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/autobahn-20.6.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/chardet already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/python_dateutil-2.8.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pyasn1_modules-0.2.8.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/dateutil already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/Twisted-20.3.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/automat already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/twisted already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/zope already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/hamcrest already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/urllib3-1.25.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/.libs_cffi_backend already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/incremental already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/watson_developer_cloud already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pycparser-2.20.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/OpenSSL already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/autobahn already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/chardet-3.0.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/setuptools already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/Automat-20.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/attr already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/cffi already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/zope.interface-5.1.0-py3.6-nspkg.pth already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pyasn1 already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests-2.23.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/hyperlink-19.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/certifi-2020.4.5.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/six-1.15.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/txaio-20.4.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pycparser already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/cryptography already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/service_identity-18.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/constantly already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pyasn1_modules already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/setuptools-47.1.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/cffi-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/incremental-17.5.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/service_identity already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/constantly-15.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/txaio already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pyasn1-0.4.8.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/idna-2.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pyOpenSSL-19.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/easy_install.py already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/zope.interface-5.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/cryptography-2.9.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/watson_developer_cloud-1.5.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/PyHamcrest-2.0.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pkg_resources already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/_cffi_backend.cpython-36m-x86_64-linux-gnu.so already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/exampleproj already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n", "name": "stdout"}, {"output_type": "stream", "text": "Collecting pixiedust\nCollecting markdown (from pixiedust)\n  Using cached https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl\nCollecting astunparse (from pixiedust)\n  Using cached https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\nCollecting mpld3 (from pixiedust)\nCollecting requests (from pixiedust)\n  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl\nCollecting lxml (from pixiedust)\n  Using cached https://files.pythonhosted.org/packages/55/6f/c87dffdd88a54dd26a3a9fef1d14b6384a9933c455c54ce3ca7d64a84c88/lxml-4.5.1-cp36-cp36m-manylinux1_x86_64.whl\nCollecting geojson (from pixiedust)\n  Using cached https://files.pythonhosted.org/packages/e4/8d/9e28e9af95739e6d2d2f8d4bef0b3432da40b7c3588fbad4298c1be09e48/geojson-2.5.0-py2.py3-none-any.whl\nCollecting colour (from pixiedust)\n  Using cached https://files.pythonhosted.org/packages/74/46/e81907704ab203206769dee1385dc77e1407576ff8f50a0681d0a6b541be/colour-0.1.5-py2.py3-none-any.whl\nCollecting importlib-metadata; python_version < \"3.8\" (from markdown->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/98/13/a1d703ec396ade42c1d33df0e1cb691a28b7c08b336a5683912c87e04cd7/importlib_metadata-1.6.1-py2.py3-none-any.whl\nCollecting wheel<1.0,>=0.23.0 (from astunparse->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\nCollecting six<2.0,>=1.6.1 (from astunparse->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\nCollecting jinja2 (from mpld3->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl\nCollecting matplotlib (from mpld3->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/93/4b/52da6b1523d5139d04e02d9e26ceda6146b48f2a4e5d2abfdf1c7bac8c40/matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl\nCollecting certifi>=2017.4.17 (from requests->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/98/99/def511020aa8f663d4a2cfaa38467539e864799289ff354569e339e375b1/certifi-2020.4.5.2-py2.py3-none-any.whl\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl\nCollecting idna<3,>=2.5 (from requests->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl\nCollecting chardet<4,>=3.0.2 (from requests->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\nCollecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->markdown->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\nCollecting MarkupSafe>=0.23 (from jinja2->mpld3->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\nCollecting kiwisolver>=1.0.1 (from matplotlib->mpld3->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/ae/23/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl\nCollecting numpy>=1.11 (from matplotlib->mpld3->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl\nCollecting cycler>=0.10 (from matplotlib->mpld3->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\nCollecting python-dateutil>=2.1 (from matplotlib->mpld3->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\nCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->mpld3->pixiedust)\n  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n\u001b[31mpytest-openfiles 0.5.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mibm-cos-sdk-core 2.6.0 has requirement requests<2.23,>=2.18, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\nInstalling collected packages: zipp, importlib-metadata, markdown, wheel, six, astunparse, MarkupSafe, jinja2, kiwisolver, numpy, cycler, python-dateutil, pyparsing, matplotlib, mpld3, certifi, urllib3, idna, chardet, requests, lxml, geojson, colour, pixiedust\nSuccessfully installed MarkupSafe-1.1.1 astunparse-1.6.3 certifi-2020.4.5.2 chardet-3.0.4 colour-0.1.5 cycler-0.10.0 geojson-2.5.0 idna-2.9 importlib-metadata-1.6.1 jinja2-2.11.2 kiwisolver-1.2.0 lxml-4.5.1 markdown-3.2.2 matplotlib-3.2.1 mpld3-0.5.0 numpy-1.18.5 pixiedust-1.1.18 pyparsing-2.4.7 python-dateutil-2.8.1 requests-2.23.0 six-1.15.0 urllib3-1.25.9 wheel-0.34.2 zipp-3.1.0\n\u001b[31mException:\nTraceback (most recent call last):\n  File \"/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 176, in main\n    status = self.run(options, args)\n  File \"/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 441, in run\n    options.target_dir, target_temp_dir, options.upgrade\n  File \"/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 492, in _handle_target_dir\n    shutil.rmtree(target_item_dir)\n  File \"/home/spark/conda/envs/python3.6/lib/python3.6/shutil.py\", line 486, in rmtree\n    _rmtree_safe_fd(fd, path, onerror)\n  File \"/home/spark/conda/envs/python3.6/lib/python3.6/shutil.py\", line 428, in _rmtree_safe_fd\n    onerror(os.rmdir, fullname, sys.exc_info())\n  File \"/home/spark/conda/envs/python3.6/lib/python3.6/shutil.py\", line 426, in _rmtree_safe_fd\n    os.rmdir(name, dir_fd=topfd)\nOSError: [Errno 39] Directory not empty: 'linalg'\u001b[0m\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "credentials_2 = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-ce4a4a44-022d-40fe-be9b-e16e72b4153c',\n    'IBM_API_KEY_ID': 'w3-S42mlOXATWHVO_5uU5NiQ2R1EiZxELIupujtqVLIa',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n    'BUCKET': 'bdaproject-donotdelete-pr-ku47cnvx97em5d',\n    'FILE': 'data.csv'\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import json\nimport watson_developer_cloud\nfrom watson_developer_cloud import NaturalLanguageUnderstandingV1\nfrom watson_developer_cloud.natural_language_understanding_v1 \\\n  import Features, EntitiesOptions, KeywordsOptions\n    \nimport ibm_boto3\nfrom botocore.client import Config\n\nimport re\nimport nltk\nfrom nltk import word_tokenize,sent_tokenize,ne_chunk\n\ncos = ibm_boto3.client('s3',\n                    ibm_api_key_id=credentials_2['IBM_API_KEY_ID'],\n                    ibm_service_instance_id=credentials_2['IAM_SERVICE_ID'],\n                    ibm_auth_endpoint=credentials_2['IBM_AUTH_ENDPOINT'],\n                    config=Config(signature_version='oauth'),\n                    endpoint_url=credentials_2['ENDPOINT'])\n\ndef get_file(filename):\n    '''Retrieve file from Cloud Object Storage'''\n    fileobject = cos.get_object(Bucket=credentials_2['BUCKET'], Key=filename)['Body']\n    return fileobject\n\ndef load_string(fileobject):\n    '''Load the file contents into a Python string'''\n    text = fileobject.read()\n    text = text.strip(b\"0xc7\")\n    return text.decode('latin-1')\n\ndef put_file(filename, filecontents):\n    '''Write file to Cloud Object Storage'''\n    resp = cos.put_object(Bucket=credentials_2['BUCKET'], Key=filename, Body=filecontents)\n    return resp", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sampleTextFileName = \"data.csv\"\ntext = load_string(get_file(sampleTextFileName))\ntext = text.strip(\"|'|\\\"|,|-|_|.|\"\"|<|>|/|.|(|)|\")\nnow_text = text.split('\\n')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import nltk\ntokenizer = nltk.RegexpTokenizer(r\"\\w+\")\nimport json\nfrom ibm_watson import NaturalLanguageUnderstandingV1\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\nfrom ibm_watson.natural_language_understanding_v1 import Features, CategoriesOptions,KeywordsOptions,SentimentOptions\nimport pandas as pd\n\nauthenticator = IAMAuthenticator('Sna9bingjNn5_WehY2FD3VNeRQGUjnPIDTqY4t6NpB4X')\nnatural_language_understanding = NaturalLanguageUnderstandingV1(\n    version='2019-07-12',\n    authenticator=authenticator\n)\ni=1\nprint(len(now_text))\nresponses=[]\nlst = []\nwhile i< len(now_text)-1:\n    now_text1 = now_text[i]\n    now_text1 = now_text1.strip(\"|'|<|>|/|.\")\n    i+=1\n    natural_language_understanding.set_service_url('https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/a79b8129-b17d-4fa8-8703-ca4ee7d3df6e')\n    try:\n        s = tokenizer.tokenize(now_text1.replace(\"NEWLINE_TOKEN\",'').replace(\"_\",\" \").replace(\"1\",\" \").replace(\"2\",\" \").replace(\"3\",\" \").replace(\"4\",\" \").replace(\"5\",\" \").replace(\"6\",\" \").replace(\"7\",\" \").replace(\"8\",\" \").replace(\"9\",\" \").replace(\"0\",\" \").lower())\n        listToStr = ' '.join([str(elem) for elem in s])\n        response = natural_language_understanding.analyze(\n            text=listToStr,\n            features=Features(sentiment=SentimentOptions(targets=[listToStr]))).get_result()\n        lst.append([now_text1.split(',')[0],response.get('sentiment').get('document').get('label'),response.get('sentiment').get('document').get('score')])\n        abcd = pd.DataFrame(lst, columns=['rev_id','sentiment','sentiment_score'])\n    except Exception as e:\n        print(e)\n        continue", "execution_count": null, "outputs": [{"output_type": "stream", "text": "77974\n", "name": "stdout"}, {"output_type": "stream", "text": "ERROR:root:unsupported text language: unknown\nTraceback (most recent call last):\n  File \"/home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core/base_service.py\", line 229, in send\n    response.status_code, error_message, http_response=response)\nibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: unknown, Code: 400 , X-global-transaction-id: 5cee8c51411891cc8073b15375e58306\n", "name": "stderr"}, {"output_type": "stream", "text": "Error: unsupported text language: unknown, Code: 400 , X-global-transaction-id: 5cee8c51411891cc8073b15375e58306\n", "name": "stdout"}, {"output_type": "stream", "text": "ERROR:root:unsupported text language: unknown\nTraceback (most recent call last):\n  File \"/home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core/base_service.py\", line 229, in send\n    response.status_code, error_message, http_response=response)\nibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: unknown, Code: 400 , X-global-transaction-id: df707f6c6be6460a3cff3e51fcfde1e5\n", "name": "stderr"}, {"output_type": "stream", "text": "Error: unsupported text language: unknown, Code: 400 , X-global-transaction-id: df707f6c6be6460a3cff3e51fcfde1e5\n", "name": "stdout"}, {"output_type": "stream", "text": "ERROR:root:unsupported text language: tl\nTraceback (most recent call last):\n  File \"/home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core/base_service.py\", line 229, in send\n    response.status_code, error_message, http_response=response)\nibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: tl, Code: 400 , X-global-transaction-id: c1b95a6ea36a72a8042f3b95f1223c60\n", "name": "stderr"}, {"output_type": "stream", "text": "Error: unsupported text language: tl, Code: 400 , X-global-transaction-id: c1b95a6ea36a72a8042f3b95f1223c60\n", "name": "stdout"}, {"output_type": "stream", "text": "ERROR:root:unsupported text language: da\nTraceback (most recent call last):\n  File \"/home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core/base_service.py\", line 229, in send\n    response.status_code, error_message, http_response=response)\nibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: da, Code: 400 , X-global-transaction-id: cd11684cd62d4bb725e6fd1bb4bdacf0\n", "name": "stderr"}, {"output_type": "stream", "text": "Error: unsupported text language: da, Code: 400 , X-global-transaction-id: cd11684cd62d4bb725e6fd1bb4bdacf0\n", "name": "stdout"}, {"output_type": "stream", "text": "ERROR:root:unsupported text language: la\nTraceback (most recent call last):\n  File \"/home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core/base_service.py\", line 229, in send\n    response.status_code, error_message, http_response=response)\nibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: la, Code: 400 , X-global-transaction-id: db94fecec48f5aad770dbc13d2ede4d2\n", "name": "stderr"}, {"output_type": "stream", "text": "Error: unsupported text language: la, Code: 400 , X-global-transaction-id: db94fecec48f5aad770dbc13d2ede4d2\n", "name": "stdout"}, {"output_type": "stream", "text": "ERROR:root:unsupported text language: pl\nTraceback (most recent call last):\n  File \"/home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core/base_service.py\", line 229, in send\n    response.status_code, error_message, http_response=response)\nibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: pl, Code: 400 , X-global-transaction-id: 1cade591fd85200a93a818e0adf90cfe\n", "name": "stderr"}, {"output_type": "stream", "text": "Error: unsupported text language: pl, Code: 400 , X-global-transaction-id: 1cade591fd85200a93a818e0adf90cfe\n", "name": "stdout"}, {"output_type": "stream", "text": "ERROR:root:unsupported text language: la\nTraceback (most recent call last):\n  File \"/home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core/base_service.py\", line 229, in send\n    response.status_code, error_message, http_response=response)\nibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: la, Code: 400 , X-global-transaction-id: d8ab9621c27e2ce2944f0c3acc75cbe0\n", "name": "stderr"}, {"output_type": "stream", "text": "Error: unsupported text language: la, Code: 400 , X-global-transaction-id: d8ab9621c27e2ce2944f0c3acc75cbe0\n", "name": "stdout"}, {"output_type": "stream", "text": "ERROR:root:unsupported text language: aa\nTraceback (most recent call last):\n  File \"/home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core/base_service.py\", line 229, in send\n    response.status_code, error_message, http_response=response)\nibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: aa, Code: 400 , X-global-transaction-id: d65e14d607659abbd228745ba0dbd0a0\n", "name": "stderr"}, {"output_type": "stream", "text": "Error: unsupported text language: aa, Code: 400 , X-global-transaction-id: d65e14d607659abbd228745ba0dbd0a0\n", "name": "stdout"}, {"output_type": "stream", "text": "ERROR:root:unsupported text language: micmac\nTraceback (most recent call last):\n  File \"/home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core/base_service.py\", line 229, in send\n    response.status_code, error_message, http_response=response)\nibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: micmac, Code: 400 , X-global-transaction-id: 5ca83f759f97d02449fd19179c3f9d49\n", "name": "stderr"}, {"output_type": "stream", "text": "Error: unsupported text language: micmac, Code: 400 , X-global-transaction-id: 5ca83f759f97d02449fd19179c3f9d49\n", "name": "stdout"}, {"output_type": "stream", "text": "ERROR:root:unsupported text language: la\nTraceback (most recent call last):\n  File \"/home/spark/shared/user-libs/python3.6/ibm_cloud_sdk_core/base_service.py\", line 229, in send\n    response.status_code, error_message, http_response=response)\nibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: la, Code: 400 , X-global-transaction-id: 0c3dc6cb63e7826a33bc4eb64e558764\n", "name": "stderr"}, {"output_type": "stream", "text": "Error: unsupported text language: la, Code: 400 , X-global-transaction-id: 0c3dc6cb63e7826a33bc4eb64e558764\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "abcd.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}