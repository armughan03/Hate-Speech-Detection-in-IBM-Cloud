{"cells": [{"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(spark.sparkContext, 'adcc95ed-0e6f-494f-a3a1-d5e74e038455', 'p-0e7c11d93da5347766da7d0e0924866399a71d41')\npc = project.project_context", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200616165020-0000\nKERNEL_ID = 38f05806-533a-4ddf-ade0-c64463c31cf3\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import ibmos2spark\nimport nltk\n# !pip install gensim \nfrom gensim.parsing.preprocessing import STOPWORDS", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'service_id': 'iam-ServiceId-283f9a60-2385-40f6-973d-6cd059b4731f',\n    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n    'api_key': '8-VH-6DHOLwKlgloUuzelkhXgt0elMaOQxDZB1EznoCu'\n}\n\nconfiguration_name = 'os_c42173278f6a4c65aaac64664b8be689_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf_data_1 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('data.csv', 'bdaproject-donotdelete-pr-qh7hn4gnoctssn'))", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import nltk\nalphabets = []\ntokenizer = nltk.RegexpTokenizer(r\"\\w+\")\nfor i in range(97,123):\n    alphabets.append(chr(i))\ndef cleaning(line):\n    rem_garbage = line.replace(\"NEWLINE_TOKEN\",'').replace(\"_\",\" \").replace(\"1\",\" \").replace(\"2\",\" \").replace(\"3\",\" \").replace(\"4\",\" \").replace(\"5\",\" \").replace(\"6\",\" \").replace(\"7\",\" \").replace(\"8\",\" \").replace(\"9\",\" \").replace(\"0\",\" \").lower()\n    rem_punct = tokenizer.tokenize(rem_garbage)\n    rem_stopwords = [word for word in rem_punct if not word in STOPWORDS.union(set(alphabets)) or not word.isalpha()]\n    return rem_stopwords", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def create_pair(word):\n    return (word,1)", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "coments = df_data_1.select(\"comment\").rdd.collect()", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clean_coments = []\nfor i in range(len(coments)):\n    clean_coments.extend(cleaning(coments[i][0]))", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "cleaned_comments = sc.parallelize(clean_coments)\npaired_words = cleaned_comments.map(create_pair)", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def sum_counts(a, b):\n    return a + b", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "corpus_word_count = paired_words.reduceByKey(sum_counts).collect()", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "new_df = sqlContext.createDataFrame(corpus_word_count,[\"Words\",\"Count\"]).toPandas()", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# !pip install textblob\nsentiment = []\nfrom textblob import TextBlob\nfor i in range(len(new_df)):\n    blob = TextBlob(new_df[\"Words\"][i])\n    sentiment.append((new_df[\"Words\"][i],new_df[\"Count\"][i],blob.sentiment.polarity))", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nDF = pd.DataFrame(sentiment,columns=[\"Words\",\"Count\",\"Sentiment_Score\"])", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "WCS_DF = DF[DF[\"Count\"]>20].sort_values(by=\"Count\",ascending = False)\nWCS_DF.index = range(len(WCS_DF[\"Words\"]))", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "project.save_data(\"Corpus_Word_Count.csv\", WCS_DF.to_csv(index=False),overwrite=True)", "execution_count": 15, "outputs": [{"output_type": "execute_result", "execution_count": 15, "data": {"text/plain": "{'file_name': 'Corpus_Word_Count.csv',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'bdaproject-donotdelete-pr-qh7hn4gnoctssn',\n 'asset_id': '4b650cbc-d254-4cd3-8fa4-e27b09d99819'}"}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}